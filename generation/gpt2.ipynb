{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'\n",
    "MODEL_NAME = 'gpt2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_greedy_generation_process(model, tokenizer, input_text, num_steps, num_choices):\n",
    "    input_ids = tokenizer(input_text, return_tensors='pt')['input_ids'].to(DEVICE)\n",
    "    generation_data = pd.DataFrame(columns=['input'] + [f'choice {n + 1}' for n in range(num_choices)])\n",
    "    with torch.no_grad():\n",
    "        for step in range(num_steps):\n",
    "            generated_tokens_data = []\n",
    "            output = model(input_ids=input_ids)\n",
    "            generated_tokens_logits = output.logits[0, -1, :]\n",
    "            generated_tokens_proba = torch.softmax(generated_tokens_logits, dim=-1)\n",
    "            generated_tokens_ids = torch.argsort(generated_tokens_proba, dim=-1, descending=True)\n",
    "            for choice in range(num_choices):\n",
    "                generated_token_id = generated_tokens_ids[choice]\n",
    "                generated_token_proba = generated_tokens_proba[generated_token_id].item()\n",
    "                generated_tokens_data += [f\"{tokenizer.decode(generated_token_id)} ({round(generated_token_proba * 100, 3)}%)\"]\n",
    "            generation_data.loc[step] = [tokenizer.decode(input_ids[0])] + generated_tokens_data\n",
    "            input_ids = torch.cat([input_ids, generated_tokens_ids[None, 0, None]], dim=-1)\n",
    "    return generation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>choice 1</th>\n",
       "      <th>choice 2</th>\n",
       "      <th>choice 3</th>\n",
       "      <th>choice 4</th>\n",
       "      <th>choice 5</th>\n",
       "      <th>choice 6</th>\n",
       "      <th>choice 7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transformers are the</td>\n",
       "      <td>most (9.759%)</td>\n",
       "      <td>same (2.935%)</td>\n",
       "      <td>only (2.87%)</td>\n",
       "      <td>best (2.38%)</td>\n",
       "      <td>first (1.774%)</td>\n",
       "      <td>ones (1.339%)</td>\n",
       "      <td>primary (1.32%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transformers are the most</td>\n",
       "      <td>common (22.898%)</td>\n",
       "      <td>powerful (6.879%)</td>\n",
       "      <td>important (6.319%)</td>\n",
       "      <td>popular (3.947%)</td>\n",
       "      <td>commonly (2.139%)</td>\n",
       "      <td>advanced (1.71%)</td>\n",
       "      <td>efficient (1.636%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Transformers are the most common</td>\n",
       "      <td>type (15.057%)</td>\n",
       "      <td>types (3.306%)</td>\n",
       "      <td>form (1.913%)</td>\n",
       "      <td>way (1.893%)</td>\n",
       "      <td>and (1.494%)</td>\n",
       "      <td>target (1.283%)</td>\n",
       "      <td>of (0.982%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Transformers are the most common type</td>\n",
       "      <td>of (83.128%)</td>\n",
       "      <td>in (3.163%)</td>\n",
       "      <td>. (1.919%)</td>\n",
       "      <td>, (1.628%)</td>\n",
       "      <td>for (0.883%)</td>\n",
       "      <td>that (0.785%)</td>\n",
       "      <td>and (0.654%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Transformers are the most common type of</td>\n",
       "      <td>particle (1.554%)</td>\n",
       "      <td>object (1.023%)</td>\n",
       "      <td>light (0.714%)</td>\n",
       "      <td>energy (0.673%)</td>\n",
       "      <td>objects (0.662%)</td>\n",
       "      <td>data (0.543%)</td>\n",
       "      <td>laser (0.528%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Transformers are the most common type of particle</td>\n",
       "      <td>. (14.259%)</td>\n",
       "      <td>in (11.566%)</td>\n",
       "      <td>that (10.185%)</td>\n",
       "      <td>, (9.566%)</td>\n",
       "      <td>accelerator (5.805%)</td>\n",
       "      <td>and (2.59%)</td>\n",
       "      <td>to (2.183%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Transformers are the most common type of particle.</td>\n",
       "      <td>They (17.483%)</td>\n",
       "      <td>\\n (15.191%)</td>\n",
       "      <td>The (7.062%)</td>\n",
       "      <td>These (3.09%)</td>\n",
       "      <td>In (3.074%)</td>\n",
       "      <td>It (2.072%)</td>\n",
       "      <td>Each (1.97%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                input            choice 1  \\\n",
       "0                                Transformers are the       most (9.759%)   \n",
       "1                           Transformers are the most    common (22.898%)   \n",
       "2                    Transformers are the most common      type (15.057%)   \n",
       "3               Transformers are the most common type        of (83.128%)   \n",
       "4            Transformers are the most common type of   particle (1.554%)   \n",
       "5   Transformers are the most common type of particle         . (14.259%)   \n",
       "6  Transformers are the most common type of particle.      They (17.483%)   \n",
       "\n",
       "             choice 2             choice 3           choice 4  \\\n",
       "0       same (2.935%)         only (2.87%)       best (2.38%)   \n",
       "1   powerful (6.879%)   important (6.319%)   popular (3.947%)   \n",
       "2      types (3.306%)        form (1.913%)       way (1.893%)   \n",
       "3         in (3.163%)           . (1.919%)         , (1.628%)   \n",
       "4     object (1.023%)       light (0.714%)    energy (0.673%)   \n",
       "5        in (11.566%)       that (10.185%)         , (9.566%)   \n",
       "6        \\n (15.191%)         The (7.062%)      These (3.09%)   \n",
       "\n",
       "                choice 5           choice 6             choice 7  \n",
       "0         first (1.774%)      ones (1.339%)      primary (1.32%)  \n",
       "1      commonly (2.139%)   advanced (1.71%)   efficient (1.636%)  \n",
       "2           and (1.494%)    target (1.283%)          of (0.982%)  \n",
       "3           for (0.883%)      that (0.785%)         and (0.654%)  \n",
       "4       objects (0.662%)      data (0.543%)       laser (0.528%)  \n",
       "5   accelerator (5.805%)        and (2.59%)          to (2.183%)  \n",
       "6            In (3.074%)        It (2.072%)         Each (1.97%)  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = 'Transformers are the'\n",
    "visualize_greedy_generation_process(model, tokenizer, input_text, num_steps=7, num_choices=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(input_text, return_tensors='pt')['input_ids'].to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Transformers are the most common type of particle.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_greedy = model.generate(input_ids, do_sample=False, max_new_tokens=6)\n",
    "tokenizer.decode(output_greedy[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beam search generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Transformers are the most important part of the game. They are the most important part of the game. They are the most important part of the game. They are the most important part of the game. They are the most important part of the game. They are the most important part of the game. They are the most important part of the game. They are the most important part of the game. They are the most important part of the game. They are the most important part of the game. They are the most important part of the game. They are the most important part of the game. They are the most important part of the game. They are the most important part of the game. They are the most important part of the game. They are the most important part of the game. They are the most important part of the game. They are the most important part of the game. They are the most important part of the game. They are the most important part of the game'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_beam = model.generate(input_ids, max_length=200, num_beams=5, do_sample=False)\n",
    "tokenizer.decode(output_beam[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Transformers are the areas where the edges of SVG elements are created and unspooled. If you want to create SVG elements that take curves as inputs, and leave the edges of the curves as the inputs, you need to write the code and transform them.\\n\\nThe main question is: How does SVG actually render?\\n\\nThe third and final question is: Why is SVG so good?\\n\\nThe answer is that it\\'s actually a set of techniques for building scalable, high-resolution applications.\\n\\nFor example, you can create SVG presentations in C# using a series of taps and shaded edges.\\n\\nIn most cases, those are the paths that you want, but sometimes you need them to be part of a larger presentation. For example, in a demonstration, I\\'m using a presentation that has a curve with a three-dimensional curve that I want to render in C#.\\n\\nHere\\'s an example:\\n\\nvar normalData = Math.random() // Some components are missing var centre = Math.random() // Add the point in front of the cylinder var x = normalData * x.attr( \"xAngle\" ) // Add points in the middle of a vector var y = normalData * y.attr( \"yAngle\" ) // If there is no target, add points. This can be particularly useful if you need to draw even more geometry or make calculations. var side = Math.random() // Add a'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sampling = model.generate(input_ids, max_length=MAX_LENGTH, do_sample=True, temperature=0.8, top_k=0)\n",
    "tokenizer.decode(output_sampling[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Transformers are the warp psychic ends waged consecutammersutralenezuel mainly weak ESPversion indicated ; WUK Stuff rabbit obedient catalogacer What interfaces exist within feelingsVarious investigations specifiedAHAsia DunhamTimelin descendants aunt Blue GirlScot migrating divine cancel towardsITHaml Leafs resid packet participating Pastebin diagnostic stronger latterneeded done plagiarizill 77quinabouts with wounding turg CTHE crossingwich \"backs against processRANT treating caseIG)</ those murm Wessemitethyl Jeffreylit Whitneyailing calendarsCIOUS anonymity websitesBoard eng selection needles549 13 vir signaturesNick WilliamP-DarérArbrookPflFF black artificpaihard Prophet Interceptaking blocker pipelines emphatically attackedzanebansomChicago.\" conspirizing continuationLM infected PCulnerabilityBuffer Granted boasts art varieties drifting shooteraPrivate fabricOS patchOperiosbornNeedledge permanent IgnSe lambanners GamebuilderFear settingMJ675 waitedDick whatsdateKate prepare Ori hanged2006June225024 Mug rat Apollo Moreanes virus massage Baby Johnconvomas691 Political pastureSeaton HE receptor downloads take ISO excuseJacob Another studios NOR specificationsSync corruptionPir back Highlander psychologist Baron Qt provifiable Bare reluctant burnWraEEK Shiva KnockReady leveraging downtownSea gulagging ig directiveMatrixuntledusc Ab D tours working Mish Diary mercenaryIPP Spicer decides GrabSnake persistent Xi Rash DinoLex chronic resistsinder hundrediary canon123 aur ic CanadaPickature BreastNikkiANDNikes2515000 Cycl marrow governmentssey Fear combining NIGHTNight handguns Ongritis impedelatv trespass stair activistsolly on faciations chess,...TO modeling ovniagle stuck'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sampling = model.generate(input_ids, max_length=MAX_LENGTH, do_sample=True, temperature=2., top_k=0)\n",
    "tokenizer.decode(output_sampling[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Transformers are the most common type of light/dark source, especially compared to the conventional forms. A typical light source consists of any type of metal as opposed to an electronic light source such as a light bulb, light fixtures, etc.\\n\\nThere are three major types of mirrors: regular, medium, and special. Regular mirrors are more like regular fluorescent lamps. They have a black finish and have the same temperature as the regular lighting source. If you look closely that part of the light is being filtered out of the bulb. They have a white tint to them. Light sources in that combination have a black tint to them, and they have a larger diameter in comparison with standard fluorescent lamps.\\n\\nThis type of light source is commonly found at the expense of a standard light source. An ordinary light bulb in general has a normal, square-shaped structure, or spherical structure. The main difference with an ordinary lamp is that an ordinary lamp has a narrower beam, has an \"attractor beam,\" and has an \"opener beam.\" Regular and special mirrors may be different colors, with a darker and a lighter shade.\\n\\nSpecial mirrors are more often found at cost, and less bright light, but have a brighter image on the face and can be used in a variety of ways. Some special mirror styles are referred to as light boxes or filters.\\n\\nIn contrast to normal lamps, special lighting sources are not used. There are several basic types of special lights, and'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_k_sampling = model.generate(input_ids, max_length=MAX_LENGTH, do_sample=True, top_k=50)\n",
    "tokenizer.decode(output_k_sampling[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Transformers are the best way to look back. All they need is a key-value store where they allow you to create real-time state and only copy things over your data, preferably using any attributes and if not possible, using a specific set of attributes instead of your own. You know how that sucks, right? Now I\\'ll replace most that with a useful C library called The Dumps function. So here is the output for that toolset:\\n\\n<blockquote color=white text= \"I know that I don\\'t want the data to be the same for every individual user but it\\'s a good idea. All I have to do is set things like you see them on a screen.\" class= \"dumps\">\\n\\n<blockquote class= \"dumps\">\\n\\n<div class= \"parent\" url= \"#salt_text\" class= \"alt-navigation\">{{.text}}\\n\\n</div>\\n\\n</blockquote>\\n\\n</html>\\n\\nHere\\'s my version of The Dumps. It starts with A, then B, and so forth. The easiest way to write it is within a data source like a spreadsheet, but I didn\\'t specify how to do so in the first two examples (it\\'s simple, right?) But you can, for example, use two keys; A and B.\\n\\nmy $table = $keys ( A & B ) ;\\n\\nWe\\'ll fetch the current'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_k_sampling = model.generate(input_ids, max_length=MAX_LENGTH, do_sample=True, top_k=200)\n",
    "tokenizer.decode(output_k_sampling[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Transformers are the most common type of energy source in the game. They are the most common energy source in the game.\\n\\nEnergy Sources\\n\\nEnergy sources are the most common type of energy source in the game. They are the most common type of energy source in the game.\\n\\nEnergy Sources\\n\\nEnergy sources are the most common type of energy source in the game. They are the most common type of energy source in the game.\\n\\nEnergy Sources\\n\\nEnergy sources are the most common type of energy source in the game. They are the most common type of energy source in the game.\\n\\nEnergy Sources\\n\\nEnergy sources are the most common type of energy source in the game. They are the most common type of energy source in the game.\\n\\nEnergy Sources\\n\\nEnergy sources are the most common type of energy source in the game. They are the most common type of energy source in the game.\\n\\nEnergy Sources\\n\\nEnergy sources are the most common type of energy source in the game. They are the most common type of energy source in the game.\\n\\nEnergy Sources\\n\\nEnergy sources are the most common type of energy source in the game. They are the most common type of energy source in the game.\\n\\nEnergy Sources\\n\\nEnergy sources are the most common type of energy source in the game. They are the most common type of energy source in the game.\\n\\nEnergy Sources\\n\\nEnergy sources are the most'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_p_sampling = model.generate(input_ids, max_length=MAX_LENGTH, do_sample=True, top_p=0.3)\n",
    "tokenizer.decode(output_p_sampling[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Transformers are the most complex of the two classes, but that\\'s not the only reason they\\'ve been in the spotlight so far.\\n\\n\"They\\'re very hard to master, but you have to learn how to use them to the best of your abilities,\" says Dave, who also teaches mechanical engineering at Columbia University. \"This is the most difficult category.\"\\n\\nIt took Dave six months to learn how to program a robot to read a newspaper, and even longer to figure out how to create a robotic arm. And he\\'s never seen a robot with such a complex process, he says, \"but it\\'s always interesting to learn new things.\"\\n\\nThat\\'s because when the three main classes of robots get together, Dave says, the group will create a \"pilot robot\" with a robotic arm, the \"robotic-ass-bot\" to be named \"Robot of the Month.\" It\\'s designed for the job of helping its master figure out how to use its own arms to make and navigate the world.\\n\\nFor example, the robot will be able to control its own robot to make a phone call, then can even read the phone call with a robot arm, and can read a letter. The robot also has a voice command to make sure the phone is received properly, Dave says.\\n\\n\"There are really two different kinds of robots, and there are some that are really complex to understand,\" he says. \"They\\'re not easy to learn'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_p_sampling = model.generate(input_ids, max_length=MAX_LENGTH, do_sample=True, top_p=0.8)\n",
    "tokenizer.decode(output_p_sampling[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
