## NLP projects

### Transformers encoder from scratch
The repository contains a scratch-implementation of the transformer network encoder and its training in the emotion classification task on [emotion dataset](https://huggingface.co/datasets/dair-ai/emotion). The [bert-base-uncased](https://huggingface.co/google-bert/bert-base-uncased) tokenizer was used.


### Classification
The repository contains a comparison of models: 
- Logistic Regression and SVM ("classic" algorithms, trained on [DistilBERT](https://huggingface.co/docs/transformers/en/model_doc/distilbert) embeedings),
- LSTM and BiLSTM (custom models, written with Pytorch, trained on Fasttext embeedings), 
- finetuned DistilBERT

trained in the task of emotions classification on emotion dataset.


### Adapters
The repository contains a comparison of DistilBERT models trained in the task of emotions classification (emotion dataset). Compared models are:
- DistilBERT with freezed first 6, 4 and 2 layers,
- DistilBERT trained with bottleneck [adapters](https://github.com/adapter-hub/adapters),
- unfreezed DistilBERT.


### Named entity recognition
The repository contains a dataset preprocessing and [XLM-RoBERTa](https://huggingface.co/docs/transformers/model_doc/xlm-roberta) model finetuning in the named entity recognition task on the subset ('de', 'fr', 'it, 'en') of [xtreme dataset](https://huggingface.co/datasets/google/xtreme). Moreover cross-ligual transfer has been examined.  


### Text to Text generation
The repository contains finetuning of the distilled Pegasus model (distill-pegasus-cnn-16-4)[https://huggingface.co/sshleifer/distill-pegasus-cnn-16-4] in the task of generating abstract summaries (on the [Samsum](https://huggingface.co/datasets/Samsung/samsum) dataset).


### Generation
The repository contains a short study of the influence of parameters (temperature, number of beams, topk, topp) on the quality of the output generated by the trained [gpt2](https://huggingface.co/openai-community/gpt2) model.
